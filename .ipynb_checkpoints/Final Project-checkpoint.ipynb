{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> 主題：英國AI跳棋 </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./ImageAndWord /explanation/03.png\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">題目簡介</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用py.game完成英國跳棋的遊戲程式，<br><br>一開始使用alpha-beta pruning來進行運算最佳棋步，<br><br>殘局的部分則使用Deep Q-learning Network (DQN)完成收拾。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./ImageAndWord /explanation/Demo.png\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">排程</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  05/22 : 編寫完成py.game：英國跳棋<br><br>05/28 : 編寫alpha-beta pruning並測試<br><br>05/31 : 與其他跳棋遊戲AI PK<br><br>06/02 : 加入殘局收拾網路(creat by DQN)<br><br>06/04 : Training it !<br><br>06/05 : 統整專案並編寫jupyter notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">alpha-beta pruning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> α-β pruning 是由Minimax Algorithm演變而來，所以我們要先從Minimax Algorithm說明。</h3>\n",
    "\n",
    "<h2> Minimax Algorithm</h2>\n",
    "\n",
    "\n",
    "<h3> Minimax Algorithm主要是用在回合制的遊戲，將所有可能的步驟求出並加以評分，<br>由於雙方目標不同分別為讓自己得最高分以及讓我們拿最低分，導致每個回合的所求相反，<br>如Figure1。<br><br>但是此方法有一個很明顯的缺點，計算量會非常龐大，可能局面亦會很多，如Figure2所示為當棋子數目給定時之所有可能局面。</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./ImageAndWord /explanation/Minimax.png\" width=600 height=600  /><a style=\"float:left;\">\n",
    "<img src=\"./ImageAndWord /explanation/number.png\" width=250 height=200 style=\"position:relative;left: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure1:α-β pruning示意圖。左手邊數字為回合數，而括號內文字為該局所求，圓形及方型中數字則為該State之Reward</h6>\n",
    "<h6>Figure2:棋子數與可走步驟圖。</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> α-β pruning</h2>\n",
    "\n",
    "<h3> α-β pruning 最重要的改變就是將一些不用算的狀態(state)給刪減掉，這樣對減少計算有極顯著的效益！<br><br>步驟一：先選取其中一個步驟\n",
    "並計算至選定的深度層。<br><br>步驟二：向上一個節點把節點下的數給算出來，並觀察該節點需求是取最大還是最小，取最小則改變該節點之β，反之亦然。<br><br>步驟三：再往上一個節點重複步驟一，如果該節點取得比同母節點之節點β(或α)小(或大)則旁枝則可不計算。<br><br>詳見figure3或reference3。<br></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./ImageAndWord /explanation/gif.gif\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure3:α-β purning流程圖，α預設值為：-∞ ； β預設為：∞，方形的目標為找最大值，圓形則為找最小值。</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Deep Q-learning Network ( DQN ) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deep Q-learning Network 是基於 Q-learning的基礎發展出來的一種Reinforced  Learning(強化學習)的方法，<br>接下來會對Q-learning做簡單的介紹。</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Q-learning </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>我們須先將所有可能的狀態及動作列出並創建出Ｑtabel，接著將當前的狀態 ( State ) 輸入 Agent 計算，<br>會利用Ｑtable在所有動作中選擇一個回報 ( Reward ) 最大的動作，並作用在環境使狀態發生改變 ( State(0)=>State(1) )，<br>但如果是很多可能的state ( 像是以照片的形式作為輸入 ) 就可能會造成維度災難！</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./ImageAndWord /explanation/prossesion.jpg\" width=400 height=400  /><a style=\"float:left;\">\n",
    "<img src=\"./ImageAndWord /explanation/prosseison2.png\" width=500 height=400 style=\"position:relative;left: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure4:Q-learning流程中英文對照圖<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./ImageAndWord /explanation/eq.jpg\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure5:Q-learning決策方程式<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> DQN </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>要解決維度災難最簡單的方法就是不要用Ｑtable或是盡量將輸入的狀態 ( State )降為成為維度更低特徵 ( feature )，<br>其中ＤＱＮ就是以Network取代了Ｑtable的方式，不需要再將所有的State列出來，<br>而是只需要將當前的狀態 ( State ) 輸入network則就會out put 相應的動作及所獲的的reward，<br>再由中選取最大的，去作用在環境上，使狀態發生改變 ( State(0)=>State(1) )。<br>其中與Q learning最不同的就是ＤＱＮ的訓練資料是來自於自己的，他會有兩個network，<br>一個為evaluate network ( 估計網路 ) 時時刻刻更新，另外一個為 target network ( 固定網路 ) 一段時間更新一次，<br>並把權重在丟到evaluate network，簡單來說就是以自己產生的參數訓練自己。</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./ImageAndWord /explanation/vs.png\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure6:Q-learning 與 DQN 之比較<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./ImageAndWord /explanation/dqn.png\" width=500 height=400  /><a style=\"float:left;\">\n",
    "<img src=\"./ImageAndWord /explanation/dqn2.png\" width=400 height=400 style=\"position:relative;left: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure7:DQN流程與中文對應圖<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">結果與討論</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>α-β pruning測試</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>我們利用 Reference 12文章裡面所提到的最受歡迎的 APP : Checkers Deluxe 用來當標靶練習，<br>結果我們的程式再與Checkers Deluxe engine 對決的表現上較該文章好，於“expert”等級取得了勝利！<br>但緊接著就發現了一個問題，平局的次數也很多，歸因於殘局所需要計算的深度太深以及我們沒有相應的殘局庫，<br>為了解決這個問題，我們引進了神經網路來學習殘局處理，減少平局增加勝局。<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./ImageAndWord /explanation/Check.png\" width=700 height=400  /><a style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure8:測試結果比較<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DQN 訓練說明</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>  為了簡化訓練，我們設定白方持兩顆棋，紅方持一顆，所有的棋都已經升變成國王。開始時，紅方的棋子會被產生在第 55 格，白方的棋子會在棋盤上的剩下 30 格被隨機產生( 31 格減掉1格一開始就會被吃掉的位置)，訓練的目標是要讓白方突破平局，得勝的要領則是要讓白方其中一隻國王佔領第 55 或第 62 格，才有機會把紅色的國王逼出來，所以在訓練的時候走到這兩格會給正的 reward 並結束一個 epsoide，同時靠近紅方的棋子、或是之後有必勝的走法(用 $\\alpha$-$\\beta$ prouning 評估)也會給正的 reward，但如果重複走同樣的棋步或下一步會被對方吃掉，則會導致 epsoide 結束並得到負的 reward，並根據被吃掉的棋子是一顆或兩顆判斷是平手或輸棋。在這樣的簡化模型下，我們只要讓神經網路學習移動就好，不會有吃子甚至連吃的問題，所以 action space 大小是有限且固定的。最後把總共的贏棋、平手、輸棋的概率列出來。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DQN 架構</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>DQN 的神經網路採用輸入層 64(對應棋盤上的64格，有白色王的格子 =2 ，紅色王的格子 =-2，其餘則 =0 )，中間兩層 256、128 的隱藏層，最後連接到 8 個神經元的輸出層，對應到 8 個可能的動作(每顆白色王可以右上、右下、左上、左下移動)，<br>optimizer 用 adam ， loss function 採 mse, 所有隱藏層都用 relu 當 activation function。<br>如果在訓練時神經網路預測出來的動作不在 available move 裡面，則重新預測一次；在測試的時候，則是看輸出的動作 array (有 8 個元素)哪個值最大且對應到符合規則的移動，就選擇該動作。<h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>DQN訓練結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>訓練的結果可以讓勝棋的概率提升到 18.07 %，被吃掉一子而被逼和的機率是 71.43 %，不過不幸的是輸棋的機率不為 0(如果用 $\\alpha$-$\\beta$ prouning 可能贏不了，但至少不會輸)，看起來是用這個訓練出來的殘局走棋方式是兩面刃呢~而且成效不是很好@@a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./ImageAndWord /explanation/f01.png\" width=600 height=500  /><a style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure9:DQN訓練結果圖，是殘局情形下去比較，故贏跟輸的機率都是在原本的基礎上新增的機率<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DQN 實戰測試</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>將之與玩家對弈，這裡用訓練好的神經網路針對當前的殘局局面給出下一步，如前面所述的，動作是看輸出的 array 哪個元素最大且符合規則而做選擇。一旦神經網路一直重複某兩步(如 30-21 然後 21-30 )，就會陷入僵局，這時我們則改用 $\\alpha$-$\\beta$ prouning 為白棋做選擇。因為我們的神經網路沒有被訓練到吃棋，所以如果下一步白棋可以吃紅棋的話，就判斷白勝，如果紅棋可以吃白棋的話，則紅勝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./ImageAndWord /explanation/f02.png\" width=400 height=300  /><a style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure10:實戰測試圖<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>實戰結論\n",
    "<h4>測試結果感覺是白棋會主動接近並壓迫紅方的棋子，如果紅方主動走到邊邊的話就會主動上來把紅棋關廁所，但也會時不時的送子造成和局甚至輸棋，<br>感覺送子之後給的負 reward 沒發揮什麼效果，也沒有防守的概念，看來用 QLN 來收殘局好像效果不是頂好的(或是我們的訓練方式或 reward 沒設計好 QQ)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>結論</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>我們在原本的ＡＩ進入殘局後增加ＤＱＮ後會有10%的機率輸棋 ( 與單純α-β pruning相比 )，但是也增加了將近 18％ 的機率贏棋，<br>其實算是不錯的進步，也是有蠻大的潛力可以讓他更好 ( 調整reward ) ，總之善用各種方式之間的配合是很重要的！<br>以我們就是以 α-β pruning 搭配 DQN， 達到的成效比單純只用ＤＱＮ還要好( 詳見Reference 14)，<br>由於α-β pruning在開局時不需要太多的算力就可以有很好的結果，ＤＱＮ也以用搭配 Encoder 及 CNN 將畫面分析並降維來完成，加快計算以及學習的成效！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Code：Github </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Code部分詳見： https://github.com/zxc54781/AI-checkers <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Reference</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center;\">原理部分</h3><br><br>\n",
    "<h6>1.Minimax : https://en.wikipedia.org/wiki/Minimax </h6>\n",
    "<h6>2.J.Schaeffer et al. , <l>Checkers Is Solved</l> , Science , Vol. 317, Issue 5844 , pp. 1518-1522 , Sep . 2007</h6>\n",
    "<h6>3.CS 161 Recitation Notes - Minimax with Alpha Beta Pruning : https://pse.is/HSWFF</h6>\n",
    "<h6>4.强化学习:Q-learning由浅入深 : https://zhuanlan.zhihu.com/p/35724704</h6>   \n",
    "<h6>5.DQN从入门到放弃 : https://zhuanlan.zhihu.com/p/21421729 </h6>\n",
    "<h6>6.An introduction to Deep Q-Learning: let’s play Doom : https://reurl.cc/7N13D</h6>\n",
    "<h6>7.强化学习—DQN算法原理详解 : https://wanjun0511.github.io/2017/11/05/DQN/ </h6>\n",
    "<h6>8.李宏毅老師DRL Lecture：https://reurl.cc/bgOYr</h6><br><br>\n",
    "<h3 style=\"text-align:center;\">程式碼部分(DQN)</h3><br><br>\n",
    "<h6>9.莫煩 PYHTON : https://morvanzhou.github.io<h6>\n",
    "<h6>10.ReinforcementLearning_by_keras : https://github.com/Jason33Wang/ReinforcementLearning_by_keras<h6>\n",
    "<h6>11.Reinforcement-Learning-five-in-a-row: https://github.com/zhijs/-Reinforcement-Learning-five-in-a-row-<h6><br>\n",
    "<h3 style=\"text-align:center;\">程式碼部分(棋盤)</h3><br><br>\n",
    "<h6>12.Deep reinforcement learning for checkers -- pretraining a policy : https://reurl.cc/8RgEd</h6>\n",
    "<h6>13.PyCheckers : https://reurl.cc/D7VRO</h6>\n",
    "<h6>14.Checkers-Reinforcement-Learning : https://reurl.cc/Mglan </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
